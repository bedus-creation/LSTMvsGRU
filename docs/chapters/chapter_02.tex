\section{Background Study}
The limitation of ANN and CNN architecture to learn sequence problem lead to the development of RNN architecture. In theory, RNNs are capable of learning sequences with long-term dependencies. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by Hochreiter\cite{hochreiter} and Bengio, et al.\cite{Bengio_2013}

\section{Literature Review}
Various study has been carried out to compare the performance of LSTM and GRU. Randomly generated non-stationary, single-valued data sequence has been used as datasets by Muccino, E. et al. \cite{eric2019} to train LSTM and GRU models with identical hyper-parameters separately. Each of this model is trained on 40 sequences, each containing 20,000 time-steps with batch size of 1. Adam optimizer has been used with learning rate of 0.001. This study indicated that the GRU model train 3.84\% faster than the LSTM model.

Shahi, T. B. et al.\cite{shahi2020stock} has done similar comparison between LSTM and GRU performance for stock market forecasting under the same conditions of hyper parameters. Further they added sentiment analysis score to the datasets to improve the prediction results. This study shows that the performance of both LSTM and GRU models are comparable under the same condition. This study has excluded to compare the training time between this two models.

In a study, conducted by Apaydin H. et al. \cite{reservoir} to predict stream-flow in dam reservoirs shows that the LSTM model has higher accuracy than the ANN model. In this research, daily stream-flow to the Ermenek hydroelectric dam reservoir located in Turkey is simulated using deep recurrent neural network (RNN) architectures. Metrics such as  correlation coefficient (CC), mean absolute error (MAE), root mean square error (RMSE), and Nash–Sutcliffe efficiency coefficient (NS), results of deep-learning architectures are compared with one another.

Khandelwal et al.\cite{khandelwal:hal-01633254} performed a similar comparison between GRU and LSTM for speech recognition acoustic models. This experiment was carried out on a large vocabulary continuous speech recognition task: transcription of TED talks. The architecture of each RNN models has used different number of layers to study the performance of model on different layers. This study has claimed that GRU outperforms LSTM in terms of (less) computation time.
